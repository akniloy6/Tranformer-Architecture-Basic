{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as f\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Self Attention Function***\n",
    "\n",
    "This is the function of a single attention head. This takes in three tensors named Q(query), K(key) & V(value). These are learnable parameters. The shapes of these tensors are (batch size, sequence_length, num_features/dimension of feature vector). These are generated using a feed forward layer which uses the embeddings embedded with positional encoding as input.\n",
    "\n",
    "Attention(Q,K,V) = softmax $(QK^T/\\sqrt d_k)V$\n",
    "\n",
    "Masking is added in the original paper, \n",
    "\n",
    "Attention(Q,K,V) = softmax $(QK^T/ \\sqrt d_k + M)V$. M is coded using a triangular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzhKcJJxv974OxWOVqUuQQ.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzhKcJJxv974OxWOVqUuQQ.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(Q: Tensor, K:Tensor, V: Tensor) -> Tensor:\n",
    "    qk = Q.bmm(K.transpose(1,2)) #matrix batchwise multiplication\n",
    "    scalling = Q.size(-1)**.5 #scalled by the square root of the number of features\n",
    "    softmax = f.softmax(qk/scalling,dim= -1) #conversion into softmax probability. values will be 0-1 via this.\n",
    "    self_attention_block = softmax.bmm(V) #MatMul again\n",
    "    return self_attention_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "q = Tensor(30,50,64)\n",
    "k = Tensor(30,50,64)\n",
    "v = Tensor(30,50,64)\n",
    "result = self_attention(q,k,v)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***Multi-head Attention Class***\n",
    "\n",
    "This takes in three tensors named Q(query), K(key) & V(value). These are then passed through a linear layer \n",
    "and then self_attention is computed paraelly. That is why they are called multi-head attention. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
