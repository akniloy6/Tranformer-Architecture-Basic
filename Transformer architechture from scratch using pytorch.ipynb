{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as f\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Self Attention Function***\n",
    "\n",
    "This is the function of a single attention head. This takes in three tensors named Q(query), K(key) & V(value). These are learnable parameters. The shapes of these tensors are (batch size, sequence_length, num_features/dimension of feature vector). These are generated using a feed forward layer which uses the embeddings embedded with positional encoding as input.\n",
    "\n",
    "Attention(Q,K,V) = softmax $(QK^T/\\sqrt d_k)V$\n",
    "\n",
    "Masking is added in the original paper, \n",
    "\n",
    "Attention(Q,K,V) = softmax $(QK^T/ \\sqrt d_k + M)V$. M is coded using a triangular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzhKcJJxv974OxWOVqUuQQ.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzhKcJJxv974OxWOVqUuQQ.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(Q: Tensor, K:Tensor, V: Tensor) -> Tensor:\n",
    "    qk = Q.bmm(K.transpose(1,2)) #matrix batchwise multiplication\n",
    "    scalling = Q.size(-1)**.5 #scalled by the square root of the number of features\n",
    "    softmax_weight = f.softmax(qk/scalling,dim= -1) #conversion into softmax probability. values will be 0-1 via this.\n",
    "    self_attention_block = softmax_weight.bmm(V) #MatMul again\n",
    "    return self_attention_block, softmax_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "q = Tensor(30,50,64)\n",
    "k = Tensor(30,50,64)\n",
    "v = Tensor(30,50,64)\n",
    "result, _ = self_attention(q,k,v)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Multi-head Attention Class***\n",
    "\n",
    "This takes in three tensors named Q(query), K(key) & V(value). These are then passed through a linear layer and then self_attention is computed paralelly. That is why they are called multi-head attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    def __init__(self, model_dimension):\n",
    "        super().__init__()\n",
    "        self.q_l = nn.Linear(model_dimension,model_dimension)\n",
    "        self.k_l = nn.Linear(model_dimension,model_dimension)\n",
    "        self.v_l = nn.Linear(model_dimension,model_dimension)\n",
    "        \n",
    "    def forward(self, q,k,v):\n",
    "        q = self.q_l(q)\n",
    "        k = self.k_l(k)\n",
    "        v = self.v_l(v)\n",
    "        single_attention_head, attention_weight = self_attention(q,k,v)\n",
    "        return single_attention_head,attention_weight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size= 30 (passing 30 sentences at once, max number of words in each sentence= 50, 64 is the vector\n",
    "#represetation for each word, model dimension D_l\n",
    "q = torch.rand(30,50,512)\n",
    "k = torch.rand(30,50,512)\n",
    "v = torch.rand(30,50,512)\n",
    "model = SingleAttentionHead(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleAttentionHead(\n",
       "  (q_l): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (k_l): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (v_l): Linear(in_features=512, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention: torch.Size([30, 50, 512])\n",
      "attention_weight: torch.Size([30, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "attention, w = model.forward(q,k,v)\n",
    "print(f\"attention: {attention.shape}\")\n",
    "print(f\"attention_weight: {w.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self,num_of_heads, model_dims):\n",
    "        super(MultiAttentionHead,self).__init__()\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.model_dims = model_dims\n",
    "        self.head = model_dims//num_of_heads\n",
    "        \n",
    "        self.attention_heads = nn.ModuleList(\n",
    "        [SingleAttentionHead(self.model_dims) for _ in range(self.num_of_heads)]\n",
    "        )\n",
    "        \n",
    "        self.linear_output = nn.Linear(self.num_of_heads *self.model_dims,self.model_dims)\n",
    "        \n",
    "    def forward(self,q,k,v):\n",
    "        attention_outputs = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        for each_attention_head in self.attention_heads:\n",
    "            output, weights = each_attention_head(q,k,v)\n",
    "            attention_outputs.append(output)\n",
    "            attention_weights.append(weights)\n",
    "        concated_output = torch.cat(attention_outputs, dim=-1)\n",
    "        print(f\"cncat_ot: {concated_output.shape}\")\n",
    "        concated_linear_output = self.linear_output(concated_output)\n",
    "        print(f\"cncat_ln_ot: {concated_linear_output.shape}\")\n",
    "        print(f\"atten_w: {attention_weights[0].shape,len(attention_weights)}\")\n",
    "        return concated_linear_output, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(30,50,512)\n",
    "k = torch.rand(30,50,512)\n",
    "v = torch.rand(30,50,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiAttentionHead(8,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cncat_ot: torch.Size([30, 50, 4096])\n",
      "cncat_ln_ot: torch.Size([30, 50, 512])\n",
      "atten_w: (torch.Size([30, 50, 50]), 8)\n"
     ]
    }
   ],
   "source": [
    "multihead_output, multihead_weight = model.forward(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
